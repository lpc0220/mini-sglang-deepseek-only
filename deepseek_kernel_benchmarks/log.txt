======================================================================
DeepSeek-R1-NVFP4-v2 Kernel Benchmarks
======================================================================
Kernels to run: 23
Batch sizes: 1,2,4,8,16,32,64,128
Sequence lengths: 128,256,512,1024,2048
Output directory: ../results/

NOTE: Each kernel runs in a separate subprocess for isolation.
      CUDA crashes in one kernel will NOT affect other kernels.
======================================================================

======================================================================
Running: rmsnorm (Norm, Memory)
======================================================================
============================================================
Benchmark: rmsnorm (Kernel #1)
============================================================

=== Decode Phase ===
  B=1: 0.0019 ms, 15.1 GB/s, 0.2% peak
  B=2: 0.0019 ms, 30.2 GB/s, 0.4% peak
  B=4: 0.0019 ms, 59.7 GB/s, 0.7% peak
  B=8: 0.0019 ms, 119.9 GB/s, 1.5% peak
  B=16: 0.0019 ms, 235.8 GB/s, 2.9% peak
  B=32: 0.0020 ms, 463.9 GB/s, 5.8% peak
  B=64: 0.0020 ms, 922.6 GB/s, 11.5% peak
  B=128: 0.0021 ms, 1778.3 GB/s, 22.2% peak

=== Prefill Phase ===
  B=1, S=128: 0.0021 ms, 1779.0 GB/s, 22.2% peak
  B=1, S=256: 0.0027 ms, 2764.1 GB/s, 34.6% peak
  B=1, S=512: 0.0046 ms, 3173.3 GB/s, 39.7% peak
  B=1, S=1024: 0.0081 ms, 3625.3 GB/s, 45.3% peak
  B=1, S=2048: 0.0150 ms, 3909.3 GB/s, 48.9% peak
  B=2, S=128: 0.0027 ms, 2750.7 GB/s, 34.4% peak
  B=2, S=256: 0.0046 ms, 3168.9 GB/s, 39.6% peak
  B=2, S=512: 0.0081 ms, 3623.8 GB/s, 45.3% peak
  B=2, S=1024: 0.0150 ms, 3911.8 GB/s, 48.9% peak
  B=2, S=2048: 0.0366 ms, 3211.9 GB/s, 40.1% peak
  B=4, S=128: 0.0046 ms, 3162.6 GB/s, 39.5% peak
  B=4, S=256: 0.0081 ms, 3622.3 GB/s, 45.3% peak
  B=4, S=512: 0.0151 ms, 3900.1 GB/s, 48.8% peak
  B=4, S=1024: 0.0366 ms, 3206.7 GB/s, 40.1% peak
  B=4, S=2048: 0.0703 ms, 3342.7 GB/s, 41.8% peak
  B=8, S=128: 0.0081 ms, 3621.8 GB/s, 45.3% peak
  B=8, S=256: 0.0150 ms, 3921.5 GB/s, 49.0% peak
  B=8, S=512: 0.0364 ms, 3225.2 GB/s, 40.3% peak
  B=8, S=1024: 0.0702 ms, 3347.4 GB/s, 41.8% peak
  B=8, S=2048: 0.1373 ms, 3421.1 GB/s, 42.8% peak
Saved ../results/rmsnorm.csv

[OK] rmsnorm completed successfully

======================================================================
Running: fused_add_rmsnorm (Norm, Memory)
======================================================================
============================================================
Benchmark: fused_add_rmsnorm (Kernel #2)
============================================================

=== Decode Phase ===
  B=1: 0.0024 ms, 24.3 GB/s, 0.3% peak
  B=2: 0.0024 ms, 48.8 GB/s, 0.6% peak
  B=4: 0.0023 ms, 97.9 GB/s, 1.2% peak
  B=8: 0.0025 ms, 185.6 GB/s, 2.3% peak
  B=16: 0.0024 ms, 375.7 GB/s, 4.7% peak
  B=32: 0.0024 ms, 758.0 GB/s, 9.5% peak
  B=64: 0.0024 ms, 1507.8 GB/s, 18.8% peak
  B=128: 0.0025 ms, 2883.8 GB/s, 36.0% peak

=== Prefill Phase ===
  B=1, S=128: 0.0025 ms, 2883.4 GB/s, 36.0% peak
  B=1, S=256: 0.0045 ms, 3275.2 GB/s, 40.9% peak
  B=1, S=512: 0.0081 ms, 3635.3 GB/s, 45.4% peak
  B=1, S=1024: 0.0143 ms, 4109.9 GB/s, 51.4% peak
  B=1, S=2048: 0.0272 ms, 4320.7 GB/s, 54.0% peak
  B=2, S=128: 0.0045 ms, 3243.2 GB/s, 40.5% peak
  B=2, S=256: 0.0081 ms, 3636.6 GB/s, 45.5% peak
  B=2, S=512: 0.0142 ms, 4134.8 GB/s, 51.7% peak
  B=2, S=1024: 0.0273 ms, 4305.6 GB/s, 53.8% peak
  B=2, S=2048: 0.0676 ms, 3475.9 GB/s, 43.4% peak
  B=4, S=128: 0.0081 ms, 3633.2 GB/s, 45.4% peak
  B=4, S=256: 0.0143 ms, 4104.4 GB/s, 51.3% peak
  B=4, S=512: 0.0269 ms, 4367.9 GB/s, 54.6% peak
  B=4, S=1024: 0.0674 ms, 3484.7 GB/s, 43.6% peak
  B=4, S=2048: 0.1319 ms, 3562.1 GB/s, 44.5% peak
  B=8, S=128: 0.0143 ms, 4109.2 GB/s, 51.4% peak
  B=8, S=256: 0.0275 ms, 4270.2 GB/s, 53.4% peak
  B=8, S=512: 0.0676 ms, 3474.6 GB/s, 43.4% peak
  B=8, S=1024: 0.1314 ms, 3574.1 GB/s, 44.7% peak
  B=8, S=2048: 0.2607 ms, 3603.7 GB/s, 45.0% peak
Saved ../results/fused_add_rmsnorm.csv

[OK] fused_add_rmsnorm completed successfully

======================================================================
Running: cutlass_scaled_fp4_mm (GEMM, Compute)
======================================================================
============================================================
Benchmark: cutlass_scaled_fp4_mm (Kernel #3)
============================================================

=== Decode Phase ===
  q_b_proj B=1: 0.0064 ms, 11858.2 GFLOPS, 0.13% peak
  kv_b_proj B=1: 0.0051 ms, 6566.9 GFLOPS, 0.07% peak
  o_proj B=1: 0.0233 ms, 10068.7 GFLOPS, 0.11% peak
  q_b_proj B=2: 0.0064 ms, 23542.0 GFLOPS, 0.26% peak
  kv_b_proj B=2: 0.0051 ms, 13200.1 GFLOPS, 0.15% peak
  o_proj B=2: 0.0233 ms, 20189.7 GFLOPS, 0.22% peak
  q_b_proj B=4: 0.0063 ms, 48143.0 GFLOPS, 0.53% peak
  kv_b_proj B=4: 0.0051 ms, 26447.1 GFLOPS, 0.29% peak
  o_proj B=4: 0.0233 ms, 40335.0 GFLOPS, 0.45% peak
  q_b_proj B=8: 0.0063 ms, 95630.9 GFLOPS, 1.06% peak
  kv_b_proj B=8: 0.0051 ms, 52880.3 GFLOPS, 0.59% peak
  o_proj B=8: 0.0233 ms, 80539.9 GFLOPS, 0.89% peak
  q_b_proj B=16: 0.0063 ms, 192645.3 GFLOPS, 2.14% peak
  kv_b_proj B=16: 0.0050 ms, 106877.9 GFLOPS, 1.19% peak
  o_proj B=16: 0.0233 ms, 160983.0 GFLOPS, 1.79% peak
  q_b_proj B=32: 0.0063 ms, 384490.8 GFLOPS, 4.27% peak
  kv_b_proj B=32: 0.0050 ms, 215615.2 GFLOPS, 2.40% peak
  o_proj B=32: 0.0234 ms, 320572.8 GFLOPS, 3.56% peak
  q_b_proj B=64: 0.0064 ms, 757802.6 GFLOPS, 8.42% peak
  kv_b_proj B=64: 0.0050 ms, 427340.8 GFLOPS, 4.75% peak
  o_proj B=64: 0.0234 ms, 641426.6 GFLOPS, 7.13% peak
  q_b_proj B=128: 0.0068 ms, 1430711.0 GFLOPS, 15.90% peak
  kv_b_proj B=128: 0.0054 ms, 792419.9 GFLOPS, 8.80% peak
  o_proj B=128: 0.0240 ms, 1254485.7 GFLOPS, 13.94% peak

=== Prefill Phase ===
  q_b_proj B=1, S=128: 0.0068 ms, 1427473.3 GFLOPS, 15.86% peak
  kv_b_proj B=1, S=128: 0.0054 ms, 792883.5 GFLOPS, 8.81% peak
  o_proj B=1, S=128: 0.0239 ms, 1256999.6 GFLOPS, 13.97% peak
  q_b_proj B=1, S=256: 0.0091 ms, 2115182.5 GFLOPS, 23.50% peak
  kv_b_proj B=1, S=256: 0.0070 ms, 1227248.2 GFLOPS, 13.64% peak
  o_proj B=1, S=256: 0.0221 ms, 2718523.7 GFLOPS, 30.21% peak
  q_b_proj B=1, S=512: 0.0126 ms, 3077001.2 GFLOPS, 34.19% peak
  kv_b_proj B=1, S=512: 0.0102 ms, 1688249.9 GFLOPS, 18.76% peak
  o_proj B=1, S=512: 0.0236 ms, 5103555.2 GFLOPS, 56.71% peak
  q_b_proj B=1, S=1024: 0.0200 ms, 3870839.2 GFLOPS, 43.01% peak
  kv_b_proj B=1, S=1024: 0.0152 ms, 2261887.4 GFLOPS, 25.13% peak
  o_proj B=1, S=1024: 0.0512 ms, 4699512.8 GFLOPS, 52.22% peak
  q_b_proj B=1, S=2048: 0.0397 ms, 3889929.9 GFLOPS, 43.22% peak
  kv_b_proj B=1, S=2048: 0.0282 ms, 2436079.5 GFLOPS, 27.07% peak
  o_proj B=1, S=2048: 0.0911 ms, 5280735.9 GFLOPS, 58.67% peak
  q_b_proj B=2, S=128: 0.0092 ms, 2110014.9 GFLOPS, 23.44% peak
  kv_b_proj B=2, S=128: 0.0071 ms, 1217908.0 GFLOPS, 13.53% peak
  o_proj B=2, S=128: 0.0228 ms, 2634627.0 GFLOPS, 29.27% peak
  q_b_proj B=2, S=256: 0.0125 ms, 3081717.3 GFLOPS, 34.24% peak
  kv_b_proj B=2, S=256: 0.0102 ms, 1688362.7 GFLOPS, 18.76% peak
  o_proj B=2, S=256: 0.0234 ms, 5148441.5 GFLOPS, 57.20% peak
  q_b_proj B=2, S=512: 0.0200 ms, 3870710.6 GFLOPS, 43.01% peak
  kv_b_proj B=2, S=512: 0.0153 ms, 2247932.3 GFLOPS, 24.98% peak
  o_proj B=2, S=512: 0.0495 ms, 4862640.7 GFLOPS, 54.03% peak
  q_b_proj B=2, S=1024: 0.0400 ms, 3864632.8 GFLOPS, 42.94% peak
  kv_b_proj B=2, S=1024: 0.0284 ms, 2422421.2 GFLOPS, 26.92% peak
  o_proj B=2, S=1024: 0.0954 ms, 5042381.1 GFLOPS, 56.03% peak
  q_b_proj B=2, S=2048: 0.0761 ms, 4065717.2 GFLOPS, 45.17% peak
  kv_b_proj B=2, S=2048: 0.0528 ms, 2604606.5 GFLOPS, 28.94% peak
  o_proj B=2, S=2048: 0.1822 ms, 5278991.4 GFLOPS, 58.66% peak
  q_b_proj B=4, S=128: 0.0127 ms, 3045632.7 GFLOPS, 33.84% peak
  kv_b_proj B=4, S=128: 0.0102 ms, 1687287.7 GFLOPS, 18.75% peak
  o_proj B=4, S=128: 0.0255 ms, 4720862.1 GFLOPS, 52.45% peak
  q_b_proj B=4, S=256: 0.0199 ms, 3886300.7 GFLOPS, 43.18% peak
  kv_b_proj B=4, S=256: 0.0148 ms, 2322372.0 GFLOPS, 25.80% peak
  o_proj B=4, S=256: 0.0500 ms, 4813895.1 GFLOPS, 53.49% peak
  q_b_proj B=4, S=512: 0.0396 ms, 3903089.2 GFLOPS, 43.37% peak
  kv_b_proj B=4, S=512: 0.0283 ms, 2428039.5 GFLOPS, 26.98% peak
  o_proj B=4, S=512: 0.0951 ms, 5056639.3 GFLOPS, 56.18% peak
  q_b_proj B=4, S=1024: 0.0757 ms, 4085520.3 GFLOPS, 45.39% peak
  kv_b_proj B=4, S=1024: 0.0532 ms, 2581881.1 GFLOPS, 28.69% peak
  o_proj B=4, S=1024: 0.1827 ms, 5266941.3 GFLOPS, 58.52% peak
  q_b_proj B=4, S=2048: 0.1470 ms, 4205992.9 GFLOPS, 46.73% peak
  kv_b_proj B=4, S=2048: 0.1025 ms, 2682138.6 GFLOPS, 29.80% peak
  o_proj B=4, S=2048: 0.3583 ms, 5369486.3 GFLOPS, 59.66% peak
  q_b_proj B=8, S=128: 0.0203 ms, 3816562.6 GFLOPS, 42.41% peak
  kv_b_proj B=8, S=128: 0.0149 ms, 2307588.0 GFLOPS, 25.64% peak
  o_proj B=8, S=128: 0.0501 ms, 4798686.5 GFLOPS, 53.32% peak
  q_b_proj B=8, S=256: 0.0399 ms, 3871387.5 GFLOPS, 43.02% peak
  kv_b_proj B=8, S=256: 0.0284 ms, 2422115.2 GFLOPS, 26.91% peak
  o_proj B=8, S=256: 0.0934 ms, 5151525.0 GFLOPS, 57.24% peak
  q_b_proj B=8, S=512: 0.0752 ms, 4114483.8 GFLOPS, 45.72% peak
  kv_b_proj B=8, S=512: 0.0541 ms, 2540803.6 GFLOPS, 28.23% peak
  o_proj B=8, S=512: 0.1847 ms, 5209481.2 GFLOPS, 57.88% peak
  q_b_proj B=8, S=1024: 0.1459 ms, 4238651.0 GFLOPS, 47.10% peak
  kv_b_proj B=8, S=1024: 0.1016 ms, 2706657.7 GFLOPS, 30.07% peak
  o_proj B=8, S=1024: 0.3605 ms, 5337558.9 GFLOPS, 59.31% peak
  q_b_proj B=8, S=2048: 0.2958 ms, 4181895.8 GFLOPS, 46.47% peak
  kv_b_proj B=8, S=2048: 0.2053 ms, 2677667.7 GFLOPS, 29.75% peak
  o_proj B=8, S=2048: 0.7177 ms, 5361834.4 GFLOPS, 59.58% peak
Saved ../results/cutlass_scaled_fp4_mm.csv

[OK] cutlass_scaled_fp4_mm completed successfully

======================================================================
Running: dsv3_fused_a_gemm (GEMM, Compute)
======================================================================
============================================================
Benchmark: dsv3_fused_a_gemm (Kernel #4)
============================================================

=== Decode Phase (B<=16, low-latency path) ===
  B=1: 0.0051 ms, 5944.2 GFLOPS, 0.26% peak
  B=2: 0.0051 ms, 11905.5 GFLOPS, 0.53% peak
  B=4: 0.0051 ms, 23713.6 GFLOPS, 1.05% peak
  B=8: 0.0052 ms, 46818.6 GFLOPS, 2.08% peak
  B=16: 0.0058 ms, 83791.8 GFLOPS, 3.72% peak
Saved ../results/dsv3_fused_a_gemm.csv

[OK] dsv3_fused_a_gemm completed successfully

======================================================================
Running: dsv3_router_gemm (GEMM, Compute)
======================================================================
============================================================
Benchmark: dsv3_router_gemm (Kernel #5)
============================================================

=== Decode Phase ===
  B=1: 0.0020 ms, 1793.5 GFLOPS, 0.08% peak
  B=2: 0.0023 ms, 3214.9 GFLOPS, 0.14% peak
  B=4: 0.0027 ms, 5367.8 GFLOPS, 0.24% peak
  B=8: 0.0036 ms, 8177.3 GFLOPS, 0.36% peak
  B=16: 0.0071 ms, 8276.5 GFLOPS, 0.37% peak
  Skipping B=32: kernel limited to num_tokens <= 16
  Skipping B=64: kernel limited to num_tokens <= 16
  Skipping B=128: kernel limited to num_tokens <= 16

=== Prefill Phase (skipped - kernel limited to 16 tokens) ===
Saved ../results/dsv3_router_gemm.csv

[OK] dsv3_router_gemm completed successfully

======================================================================
Running: bmm_fp8 (BMM, Compute)
======================================================================
============================================================
Benchmark: bmm_fp8 (Kernel #6)
============================================================

=== Decode Phase: q_nope * w_kc ===
  B=1: 0.0055 ms, 3057.2 GFLOPS, 0.07% peak
  B=2: 0.0055 ms, 6115.9 GFLOPS, 0.14% peak
  B=4: 0.0055 ms, 12209.1 GFLOPS, 0.27% peak
  B=8: 0.0055 ms, 24378.8 GFLOPS, 0.54% peak
  B=16: 0.0056 ms, 47680.0 GFLOPS, 1.06% peak
  B=32: 0.0057 ms, 93462.4 GFLOPS, 2.08% peak
  B=64: 0.0061 ms, 175747.2 GFLOPS, 3.91% peak
  B=128: 0.0069 ms, 309204.5 GFLOPS, 6.87% peak

=== Decode Phase: attn * w_vc ===
  B=1: 0.0027 ms, 6121.6 GFLOPS, 0.14% peak
  B=2: 0.0027 ms, 12229.4 GFLOPS, 0.27% peak
  B=4: 0.0027 ms, 24582.1 GFLOPS, 0.55% peak
  B=8: 0.0027 ms, 48833.1 GFLOPS, 1.09% peak
  B=16: 0.0031 ms, 85879.6 GFLOPS, 1.91% peak
  B=32: 0.0032 ms, 169123.4 GFLOPS, 3.76% peak
  B=64: 0.0033 ms, 320982.0 GFLOPS, 7.13% peak
  B=128: 0.0037 ms, 574896.0 GFLOPS, 12.78% peak
Saved ../results/bmm_fp8.csv

[OK] bmm_fp8 completed successfully

======================================================================
Running: cutlass_mla_decode (Attention, Mixed)
======================================================================
============================================================
Benchmark: cutlass_mla_decode (Kernel #7)
============================================================

=== Decode Phase (MLA Attention) ===
  B=1, seq_len=128: 0.0184 ms, 16.1 GB/s, memory
  B=1, seq_len=256: 0.0304 ms, 14.5 GB/s, memory
  B=1, seq_len=512: 0.0307 ms, 24.0 GB/s, memory
  B=1, seq_len=1024: 0.0326 ms, 40.7 GB/s, memory
  Skipping B=1, seq_len=2048: B*seq_len=2048 > 1024 (crash risk)
  B=2, seq_len=128: 0.0187 ms, 31.6 GB/s, memory
  B=2, seq_len=256: 0.0307 ms, 28.8 GB/s, memory
  B=2, seq_len=512: 0.0307 ms, 48.0 GB/s, memory
  Skipping B=2, seq_len=1024: B*seq_len=2048 > 1024 (crash risk)
  Skipping B=2, seq_len=2048: B*seq_len=4096 > 1024 (crash risk)
  B=4, seq_len=128: 0.0187 ms, 63.1 GB/s, memory
  B=4, seq_len=256: 0.0309 ms, 57.2 GB/s, memory
  Skipping B=4, seq_len=512: B*seq_len=2048 > 1024 (crash risk)
  Skipping B=4, seq_len=1024: B*seq_len=4096 > 1024 (crash risk)
  Skipping B=4, seq_len=2048: B*seq_len=8192 > 1024 (crash risk)
  B=8, seq_len=128: 0.0187 ms, 126.1 GB/s, memory
  Skipping B=8, seq_len=256: B*seq_len=2048 > 1024 (crash risk)
  Skipping B=8, seq_len=512: B*seq_len=4096 > 1024 (crash risk)
  Skipping B=8, seq_len=1024: B*seq_len=8192 > 1024 (crash risk)
  Skipping B=8, seq_len=2048: B*seq_len=16384 > 1024 (crash risk)
  Skipping B=16, seq_len=128: B*seq_len=2048 > 1024 (crash risk)
  Skipping B=16, seq_len=256: B*seq_len=4096 > 1024 (crash risk)
  Skipping B=16, seq_len=512: B*seq_len=8192 > 1024 (crash risk)
  Skipping B=16, seq_len=1024: B*seq_len=16384 > 1024 (crash risk)
  Skipping B=16, seq_len=2048: B*seq_len=32768 > 1024 (crash risk)
  Skipping B=32, seq_len=128: B*seq_len=4096 > 1024 (crash risk)
  Skipping B=32, seq_len=256: B*seq_len=8192 > 1024 (crash risk)
  Skipping B=32, seq_len=512: B*seq_len=16384 > 1024 (crash risk)
  Skipping B=32, seq_len=1024: B*seq_len=32768 > 1024 (crash risk)
  Skipping B=32, seq_len=2048: B*seq_len=65536 > 1024 (crash risk)
  Skipping B=64, seq_len=128: B*seq_len=8192 > 1024 (crash risk)
  Skipping B=64, seq_len=256: B*seq_len=16384 > 1024 (crash risk)
  Skipping B=64, seq_len=512: B*seq_len=32768 > 1024 (crash risk)
  Skipping B=64, seq_len=1024: B*seq_len=65536 > 1024 (crash risk)
  Skipping B=64, seq_len=2048: B*seq_len=131072 > 1024 (crash risk)
  Skipping B=128, seq_len=128: B*seq_len=16384 > 1024 (crash risk)
  Skipping B=128, seq_len=256: B*seq_len=32768 > 1024 (crash risk)
  Skipping B=128, seq_len=512: B*seq_len=65536 > 1024 (crash risk)
  Skipping B=128, seq_len=1024: B*seq_len=131072 > 1024 (crash risk)
  Skipping B=128, seq_len=2048: B*seq_len=262144 > 1024 (crash risk)
Saved ../results/cutlass_mla_decode.csv

[OK] cutlass_mla_decode completed successfully

======================================================================
Running: trtllm_batch_decode_with_kv_cache_mla (Attention, Mixed)
======================================================================
============================================================
Benchmark: trtllm_batch_decode_with_kv_cache_mla (Kernel #8)
============================================================

=== Decode Phase (TRT-LLM MLA Attention) ===

  Page size: 32
    B=1, seq_len=128: 0.0055 ms, 53.4 GB/s, memory
    B=1, seq_len=256: 0.0055 ms, 80.9 GB/s, memory
    B=1, seq_len=512: 0.0074 ms, 99.8 GB/s, memory
    B=1, seq_len=1024: 0.0099 ms, 134.1 GB/s, memory
    B=1, seq_len=2048: 0.0113 ms, 221.3 GB/s, memory
    B=2, seq_len=128: 0.0059 ms, 100.2 GB/s, memory
    B=2, seq_len=256: 0.0060 ms, 147.5 GB/s, memory
    B=2, seq_len=512: 0.0081 ms, 182.2 GB/s, memory
    B=2, seq_len=1024: 0.0102 ms, 259.1 GB/s, memory
    B=2, seq_len=2048: 0.0126 ms, 397.3 GB/s, memory
    B=4, seq_len=128: 0.0068 ms, 172.3 GB/s, memory
    B=4, seq_len=256: 0.0068 ms, 259.2 GB/s, memory
    B=4, seq_len=512: 0.0090 ms, 327.6 GB/s, memory
    B=4, seq_len=1024: 0.0116 ms, 459.0 GB/s, memory
    B=4, seq_len=2048: 0.0163 ms, 615.5 GB/s, memory
    B=8, seq_len=128: 0.0076 ms, 311.1 GB/s, memory
    B=8, seq_len=256: 0.0076 ms, 467.6 GB/s, memory
    B=8, seq_len=512: 0.0104 ms, 567.8 GB/s, memory
    B=8, seq_len=1024: 0.0156 ms, 682.3 GB/s, memory
    B=8, seq_len=2048: 0.0232 ms, 863.3 GB/s, memory
    B=16, seq_len=128: 0.0094 ms, 503.6 GB/s, memory
    B=16, seq_len=256: 0.0093 ms, 765.2 GB/s, memory
    B=16, seq_len=512: 0.0131 ms, 903.3 GB/s, memory
    B=16, seq_len=1024: 0.0201 ms, 1056.5 GB/s, memory
    B=16, seq_len=2048: 0.0158 ms, 2538.5 GB/s, memory
    B=32, seq_len=128: 0.0074 ms, 1268.6 GB/s, memory
    B=32, seq_len=256: 0.0089 ms, 1585.8 GB/s, memory
    B=32, seq_len=512: 0.0141 ms, 1671.7 GB/s, memory
    B=32, seq_len=1024: 0.0171 ms, 2488.1 GB/s, memory
    B=32, seq_len=2048: 0.0242 ms, 3311.8 GB/s, memory
    B=64, seq_len=128: 0.0081 ms, 2321.6 GB/s, memory
    B=64, seq_len=256: 0.0100 ms, 2844.7 GB/s, memory
    B=64, seq_len=512: 0.0129 ms, 3660.7 GB/s, memory
    B=64, seq_len=1024: 0.0195 ms, 4363.3 GB/s, memory
    B=64, seq_len=2048: 0.0444 ms, 3610.9 GB/s, memory
    B=128, seq_len=128: 0.0127 ms, 2979.4 GB/s, memory
    B=128, seq_len=256: 0.0172 ms, 3288.1 GB/s, memory
    B=128, seq_len=512: 0.0246 ms, 3841.5 GB/s, memory
    B=128, seq_len=1024: 0.0463 ms, 3669.5 GB/s, memory
    B=128, seq_len=2048: 0.0854 ms, 3759.3 GB/s, memory

  Page size: 64
    B=1, seq_len=128: 0.0055 ms, 53.9 GB/s, memory
    B=1, seq_len=256: 0.0054 ms, 81.2 GB/s, memory
    B=1, seq_len=512: 0.0074 ms, 99.2 GB/s, memory
    B=1, seq_len=1024: 0.0098 ms, 135.5 GB/s, memory
    B=1, seq_len=2048: 0.0111 ms, 226.8 GB/s, memory
    B=2, seq_len=128: 0.0057 ms, 102.7 GB/s, memory
    B=2, seq_len=256: 0.0056 ms, 157.7 GB/s, memory
    B=2, seq_len=512: 0.0077 ms, 192.4 GB/s, memory
    B=2, seq_len=1024: 0.0097 ms, 273.2 GB/s, memory
    B=2, seq_len=2048: 0.0115 ms, 437.4 GB/s, memory
    B=4, seq_len=128: 0.0063 ms, 186.0 GB/s, memory
    B=4, seq_len=256: 0.0064 ms, 276.4 GB/s, memory
    B=4, seq_len=512: 0.0083 ms, 355.4 GB/s, memory
    B=4, seq_len=1024: 0.0109 ms, 487.3 GB/s, memory
    B=4, seq_len=2048: 0.0143 ms, 699.3 GB/s, memory
    B=8, seq_len=128: 0.0069 ms, 339.9 GB/s, memory
    B=8, seq_len=256: 0.0069 ms, 515.5 GB/s, memory
    B=8, seq_len=512: 0.0094 ms, 628.9 GB/s, memory
    B=8, seq_len=1024: 0.0136 ms, 783.4 GB/s, memory
    B=8, seq_len=2048: 0.0202 ms, 991.8 GB/s, memory
    B=16, seq_len=128: 0.0082 ms, 578.8 GB/s, memory
    B=16, seq_len=256: 0.0081 ms, 872.0 GB/s, memory
    B=16, seq_len=512: 0.0116 ms, 1016.4 GB/s, memory
    B=16, seq_len=1024: 0.0184 ms, 1152.4 GB/s, memory
    B=16, seq_len=2048: 0.0154 ms, 2603.4 GB/s, memory
    B=32, seq_len=128: 0.0072 ms, 1311.8 GB/s, memory
    B=32, seq_len=256: 0.0084 ms, 1678.7 GB/s, memory
    B=32, seq_len=512: 0.0139 ms, 1693.5 GB/s, memory
    B=32, seq_len=1024: 0.0169 ms, 2514.5 GB/s, memory
    B=32, seq_len=2048: 0.0239 ms, 3352.5 GB/s, memory
    B=64, seq_len=128: 0.0081 ms, 2328.2 GB/s, memory
    B=64, seq_len=256: 0.0098 ms, 2892.0 GB/s, memory
    B=64, seq_len=512: 0.0126 ms, 3743.4 GB/s, memory
    B=64, seq_len=1024: 0.0192 ms, 4426.6 GB/s, memory
    B=64, seq_len=2048: 0.0442 ms, 3627.7 GB/s, memory
    B=128, seq_len=128: 0.0136 ms, 2785.6 GB/s, memory
    B=128, seq_len=256: 0.0168 ms, 3367.0 GB/s, memory
    B=128, seq_len=512: 0.0238 ms, 3962.1 GB/s, memory
    B=128, seq_len=1024: 0.0477 ms, 3563.0 GB/s, memory
    B=128, seq_len=2048: 0.0854 ms, 3755.2 GB/s, memory
Saved ../results/trtllm_batch_decode_with_kv_cache_mla.csv

[OK] trtllm_batch_decode_with_kv_cache_mla completed successfully

======================================================================
Running: trtllm_ragged_attention_deepseek (Attention, Mixed)
======================================================================
============================================================
Benchmark: trtllm_ragged_attention_deepseek (Kernel #9)
============================================================

=== Prefill Phase (TRT-LLM Ragged Attention) ===
  B=1, S=128: 0.0071 ms, 741.0 GFLOPS, memory
  B=1, S=256: 0.0104 ms, 3298.0 GFLOPS, memory
  B=1, S=512: 0.0183 ms, 4818.6 GFLOPS, memory
  B=1, S=1024: 0.0354 ms, 7683.8 GFLOPS, memory
  B=1, S=2048: 0.0833 ms, 11299.9 GFLOPS, memory
  B=2, S=128: 0.0120 ms, 1127.5 GFLOPS, memory
  B=2, S=256: 0.0177 ms, 4275.3 GFLOPS, memory
  B=2, S=512: 0.0369 ms, 5099.0 GFLOPS, memory
  B=2, S=1024: 0.0902 ms, 9179.2 GFLOPS, memory
  B=2, S=2048: 0.2552 ms, 13579.4 GFLOPS, memory
  B=4, S=128: 0.0196 ms, 1601.8 GFLOPS, memory
  B=4, S=256: 0.0341 ms, 3500.3 GFLOPS, memory
  B=4, S=512: 0.0753 ms, 6173.3 GFLOPS, memory
  B=4, S=1024: 0.1613 ms, 9574.3 GFLOPS, memory
  B=4, S=2048: 0.4965 ms, 13497.5 GFLOPS, memory
  B=8, S=128: 0.0353 ms, 1802.5 GFLOPS, memory
  B=8, S=256: 0.0602 ms, 3675.1 GFLOPS, memory
  B=8, S=512: 0.1388 ms, 5921.8 GFLOPS, memory
  B=8, S=1024: 0.3470 ms, 9121.0 GFLOPS, memory
  B=8, S=2048: 1.0047 ms, 13499.4 GFLOPS, memory
  B=16, S=128: 0.0670 ms, 1714.4 GFLOPS, memory
  B=16, S=256: 0.1171 ms, 3734.7 GFLOPS, memory
  B=16, S=512: 0.2817 ms, 6058.0 GFLOPS, memory
  B=16, S=1024: 0.7399 ms, 9509.5 GFLOPS, memory
  B=16, S=2048: 2.0131 ms, 14189.8 GFLOPS, memory
  B=32, S=128: 0.1285 ms, 1706.9 GFLOPS, memory
  B=32, S=256: 0.2324 ms, 3836.6 GFLOPS, memory
  B=32, S=512: 0.5765 ms, 6366.8 GFLOPS, memory
  B=32, S=1024: 1.4764 ms, 9881.4 GFLOPS, memory
  B=32, S=2048: 4.2598 ms, 14229.3 GFLOPS, memory
  B=64, S=128: 0.2564 ms, 1689.6 GFLOPS, memory
  B=64, S=256: 0.4590 ms, 3660.7 GFLOPS, memory
  B=64, S=512: 1.1440 ms, 6299.1 GFLOPS, memory
  B=64, S=1024: 2.9771 ms, 9913.5 GFLOPS, memory
  B=64, S=2048: 8.0851 ms, 14322.3 GFLOPS, memory
  B=128, S=128: 0.5124 ms, 1749.9 GFLOPS, memory
  B=128, S=256: 0.9117 ms, 3750.7 GFLOPS, memory
  B=128, S=512: 2.2596 ms, 6200.0 GFLOPS, memory
  B=128, S=1024: 5.7144 ms, 9819.3 GFLOPS, memory
Warning: Kernel failed for B=128, S=2048: Error in function 'aligned_alloc' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/include/flashinfer/allocator.h:49: Buffer overflow when allocating memory for trtllm_gen_softmax_workspace with size 200737792 and alignment 16, but only 125829120 bytes available in AlignedAllocator. Increase the workspace buffer size.
Saved ../results/trtllm_ragged_attention_deepseek.csv

[OK] trtllm_ragged_attention_deepseek completed successfully

======================================================================
Running: mla_rope_quantize_fp8 (Attention, Memory)
======================================================================
============================================================
Benchmark: rope_quantize_fp8 (MLA config) (Kernel #10)
MLA: 128 Q heads, 1 K head, 64 rope_dim + 512 no_rope_dim
============================================================

=== MLA RoPE + FP8 Quantization ===
  tokens=128: 0.0064 ms, 4620.4 GB/s, 57.8% peak
  tokens=256: 0.0114 ms, 5094.6 GB/s, 63.7% peak
  tokens=512: 0.0233 ms, 4935.0 GB/s, 61.7% peak
  tokens=1024: 0.0467 ms, 4914.8 GB/s, 61.4% peak
  tokens=2048: 0.0919 ms, 4976.6 GB/s, 62.2% peak
  tokens=4096: 0.1833 ms, 4986.7 GB/s, 62.3% peak
  tokens=8192: 0.3697 ms, 4944.5 GB/s, 61.8% peak
  tokens=16384: 0.7491 ms, 4881.1 GB/s, 61.0% peak
  tokens=32768: 1.5405 ms, 4746.9 GB/s, 59.3% peak
  tokens=65536: 3.0771 ms, 4753.0 GB/s, 59.4% peak
  tokens=131072: 6.1861 ms, 4728.5 GB/s, 59.1% peak
  tokens=262144: 13.9482 ms, 4194.3 GB/s, 52.4% peak
Saved ../results/mla_rope_quantize_fp8.csv

[OK] mla_rope_quantize_fp8 completed successfully

======================================================================
Running: apply_rope_with_cos_sin_cache_inplace (RoPE, Memory)
======================================================================
============================================================
Benchmark: apply_rope_with_cos_sin_cache_inplace (Kernel #11)
============================================================

=== Decode Phase ===
  B=1: 0.0017 ms, 352.6 GB/s, 4.4% peak
  B=2: 0.0017 ms, 384.5 GB/s, 4.8% peak
  B=4: 0.0017 ms, 460.8 GB/s, 5.8% peak
  B=8: 0.0017 ms, 608.1 GB/s, 7.6% peak
  B=16: 0.0018 ms, 887.7 GB/s, 11.1% peak
  B=32: 0.0019 ms, 1406.7 GB/s, 17.6% peak
  B=64: 0.0022 ms, 2169.5 GB/s, 27.1% peak
  B=128: 0.0033 ms, 2723.6 GB/s, 34.0% peak

=== Prefill Phase ===
  B=1, S=128: 0.0033 ms, 2739.8 GB/s, 34.2% peak
  B=1, S=256: 0.0046 ms, 3755.0 GB/s, 46.9% peak
  B=1, S=512: 0.0073 ms, 4665.2 GB/s, 58.3% peak
  B=1, S=1024: 0.0128 ms, 5288.4 GB/s, 66.1% peak
  B=1, S=2048: 0.0243 ms, 5548.6 GB/s, 69.4% peak
  B=2, S=128: 0.0046 ms, 3732.6 GB/s, 46.7% peak
  B=2, S=256: 0.0073 ms, 4654.9 GB/s, 58.2% peak
  B=2, S=512: 0.0128 ms, 5284.4 GB/s, 66.1% peak
  B=2, S=1024: 0.0247 ms, 5455.8 GB/s, 68.2% peak
  B=2, S=2048: 0.0675 ms, 3987.3 GB/s, 49.8% peak
  B=4, S=128: 0.0073 ms, 4657.5 GB/s, 58.2% peak
  B=4, S=256: 0.0128 ms, 5301.7 GB/s, 66.3% peak
  B=4, S=512: 0.0241 ms, 5588.4 GB/s, 69.9% peak
  B=4, S=1024: 0.0674 ms, 3989.5 GB/s, 49.9% peak
  B=4, S=2048: 0.1378 ms, 3901.3 GB/s, 48.8% peak
  B=8, S=128: 0.0127 ms, 5315.2 GB/s, 66.4% peak
  B=8, S=256: 0.0242 ms, 5559.3 GB/s, 69.5% peak
  B=8, S=512: 0.0670 ms, 4013.8 GB/s, 50.2% peak
  B=8, S=1024: 0.1375 ms, 3909.9 GB/s, 48.9% peak
  B=8, S=2048: 0.2838 ms, 3786.0 GB/s, 47.3% peak
Saved ../results/apply_rope_with_cos_sin_cache_inplace.csv

[OK] apply_rope_with_cos_sin_cache_inplace completed successfully

======================================================================
Running: concat_mla_k (Concat, Memory)
======================================================================
============================================================
Benchmark: concat_mla_k (Kernel #12)
============================================================

=== Decode Phase ===
  B=1: 0.0021 ms, 38.6 GB/s, 0.5% peak
  B=2: 0.0030 ms, 54.3 GB/s, 0.7% peak
  B=4: 0.0048 ms, 68.1 GB/s, 0.9% peak
  B=8: 0.0048 ms, 135.7 GB/s, 1.7% peak
  B=16: 0.0049 ms, 270.4 GB/s, 3.4% peak
  B=32: 0.0049 ms, 540.5 GB/s, 6.8% peak
  B=64: 0.0049 ms, 1075.7 GB/s, 13.4% peak
  B=128: 0.0049 ms, 2143.8 GB/s, 26.8% peak

=== Prefill Phase ===
  B=1, S=128: 0.0049 ms, 2144.1 GB/s, 26.8% peak
  B=1, S=256: 0.0049 ms, 4252.7 GB/s, 53.2% peak
  B=1, S=512: 0.0055 ms, 7615.9 GB/s, 95.2% peak
  B=1, S=1024: 0.0106 ms, 7889.2 GB/s, 98.6% peak
  B=1, S=2048: 0.0274 ms, 6132.3 GB/s, 76.7% peak
  B=2, S=128: 0.0050 ms, 4190.7 GB/s, 52.4% peak
  B=2, S=256: 0.0051 ms, 8284.9 GB/s, 103.6% peak
  B=2, S=512: 0.0095 ms, 8799.5 GB/s, 110.0% peak
  B=2, S=1024: 0.0274 ms, 6128.4 GB/s, 76.6% peak
  B=2, S=2048: 0.0521 ms, 6454.4 GB/s, 80.7% peak
  B=4, S=128: 0.0051 ms, 8286.2 GB/s, 103.6% peak
  B=4, S=256: 0.0095 ms, 8841.4 GB/s, 110.5% peak
  B=4, S=512: 0.0273 ms, 6164.0 GB/s, 77.0% peak
  B=4, S=1024: 0.0523 ms, 6427.8 GB/s, 80.3% peak
  B=4, S=2048: 0.1009 ms, 6659.0 GB/s, 83.2% peak
  B=8, S=128: 0.0097 ms, 8690.3 GB/s, 108.6% peak
  B=8, S=256: 0.0269 ms, 6242.7 GB/s, 78.0% peak
  B=8, S=512: 0.0524 ms, 6410.6 GB/s, 80.1% peak
  B=8, S=1024: 0.1010 ms, 6655.7 GB/s, 83.2% peak
  B=8, S=2048: 0.1984 ms, 6774.7 GB/s, 84.7% peak
Saved ../results/concat_mla_k.csv

[OK] concat_mla_k completed successfully

======================================================================
Running: silu_and_mul (Activation, Memory)
======================================================================
============================================================
Benchmark: silu_and_mul (Kernel #13)
============================================================

=== Decode Phase ===
  B=1: 0.0019 ms, 6.6 GB/s, 0.1% peak
  B=2: 0.0019 ms, 13.2 GB/s, 0.2% peak
  B=4: 0.0018 ms, 26.6 GB/s, 0.3% peak
  B=8: 0.0018 ms, 53.4 GB/s, 0.7% peak
  B=16: 0.0019 ms, 105.0 GB/s, 1.3% peak
  B=32: 0.0019 ms, 208.2 GB/s, 2.6% peak
  B=64: 0.0019 ms, 412.3 GB/s, 5.2% peak
  B=128: 0.0019 ms, 807.9 GB/s, 10.1% peak

=== Prefill Phase ===
  B=1, S=128: 0.0019 ms, 806.9 GB/s, 10.1% peak
  B=1, S=256: 0.0021 ms, 1472.1 GB/s, 18.4% peak
  B=1, S=512: 0.0028 ms, 2249.7 GB/s, 28.1% peak
  B=1, S=1024: 0.0038 ms, 3292.5 GB/s, 41.2% peak
  B=1, S=2048: 0.0058 ms, 4357.7 GB/s, 54.5% peak
  B=2, S=128: 0.0021 ms, 1476.9 GB/s, 18.5% peak
  B=2, S=256: 0.0028 ms, 2251.4 GB/s, 28.1% peak
  B=2, S=512: 0.0038 ms, 3299.7 GB/s, 41.2% peak
  B=2, S=1024: 0.0058 ms, 4356.3 GB/s, 54.5% peak
  B=2, S=2048: 0.0098 ms, 5123.7 GB/s, 64.0% peak
  B=4, S=128: 0.0028 ms, 2242.5 GB/s, 28.0% peak
  B=4, S=256: 0.0038 ms, 3293.8 GB/s, 41.2% peak
  B=4, S=512: 0.0058 ms, 4352.8 GB/s, 54.4% peak
  B=4, S=1024: 0.0098 ms, 5113.5 GB/s, 63.9% peak
  B=4, S=2048: 0.0197 ms, 5098.2 GB/s, 63.7% peak
  B=8, S=128: 0.0038 ms, 3287.5 GB/s, 41.1% peak
  B=8, S=256: 0.0058 ms, 4345.0 GB/s, 54.3% peak
  B=8, S=512: 0.0099 ms, 5105.7 GB/s, 63.8% peak
  B=8, S=1024: 0.0198 ms, 5078.2 GB/s, 63.5% peak
  B=8, S=2048: 0.0399 ms, 5048.8 GB/s, 63.1% peak
Saved ../results/silu_and_mul.csv

[OK] silu_and_mul completed successfully

======================================================================
Running: topk_softmax (MoE Routing, Memory)
======================================================================
============================================================
Benchmark: topk_softmax (Kernel #14)
============================================================

=== Decode Phase ===
  B=1: 0.0035 ms, 0.3 GB/s, 0.0% peak
  B=2: 0.0034 ms, 0.6 GB/s, 0.0% peak
  B=4: 0.0035 ms, 1.3 GB/s, 0.0% peak
  B=8: 0.0035 ms, 2.5 GB/s, 0.0% peak
  B=16: 0.0035 ms, 5.0 GB/s, 0.1% peak
  B=32: 0.0035 ms, 10.1 GB/s, 0.1% peak
  B=64: 0.0035 ms, 19.9 GB/s, 0.2% peak
  B=128: 0.0035 ms, 39.7 GB/s, 0.5% peak

=== Prefill Phase ===
  B=1, S=128: 0.0035 ms, 39.8 GB/s, 0.5% peak
  B=1, S=256: 0.0035 ms, 78.7 GB/s, 1.0% peak
  B=1, S=512: 0.0036 ms, 156.6 GB/s, 2.0% peak
  B=1, S=1024: 0.0039 ms, 286.4 GB/s, 3.6% peak
  B=1, S=2048: 0.0053 ms, 423.7 GB/s, 5.3% peak
  B=2, S=128: 0.0035 ms, 78.8 GB/s, 1.0% peak
  B=2, S=256: 0.0036 ms, 156.6 GB/s, 2.0% peak
  B=2, S=512: 0.0039 ms, 286.4 GB/s, 3.6% peak
  B=2, S=1024: 0.0053 ms, 424.1 GB/s, 5.3% peak
  B=2, S=2048: 0.0078 ms, 574.4 GB/s, 7.2% peak
  B=4, S=128: 0.0036 ms, 156.6 GB/s, 2.0% peak
  B=4, S=256: 0.0039 ms, 286.4 GB/s, 3.6% peak
  B=4, S=512: 0.0053 ms, 423.8 GB/s, 5.3% peak
  B=4, S=1024: 0.0078 ms, 574.5 GB/s, 7.2% peak
  B=4, S=2048: 0.0133 ms, 672.6 GB/s, 8.4% peak
  B=8, S=128: 0.0039 ms, 286.6 GB/s, 3.6% peak
  B=8, S=256: 0.0053 ms, 424.2 GB/s, 5.3% peak
  B=8, S=512: 0.0078 ms, 574.6 GB/s, 7.2% peak
  B=8, S=1024: 0.0132 ms, 672.7 GB/s, 8.4% peak
  B=8, S=2048: 0.0237 ms, 753.1 GB/s, 9.4% peak
Saved ../results/topk_softmax.csv

[OK] topk_softmax completed successfully

======================================================================
Running: topk_sigmoid (MoE Routing, Memory)
======================================================================
============================================================
Benchmark: topk_sigmoid (Kernel #15)
============================================================

=== Decode Phase ===
  B=1: 0.0037 ms, 0.3 GB/s, 0.0% peak
  B=2: 0.0037 ms, 0.6 GB/s, 0.0% peak
  B=4: 0.0037 ms, 1.2 GB/s, 0.0% peak
  B=8: 0.0037 ms, 2.3 GB/s, 0.0% peak
  B=16: 0.0037 ms, 4.7 GB/s, 0.1% peak
  B=32: 0.0037 ms, 9.4 GB/s, 0.1% peak
  B=64: 0.0038 ms, 18.5 GB/s, 0.2% peak
  B=128: 0.0038 ms, 36.9 GB/s, 0.5% peak

=== Prefill Phase ===
  B=1, S=128: 0.0038 ms, 37.0 GB/s, 0.5% peak
  B=1, S=256: 0.0038 ms, 73.3 GB/s, 0.9% peak
  B=1, S=512: 0.0038 ms, 145.9 GB/s, 1.8% peak
  B=1, S=1024: 0.0041 ms, 271.3 GB/s, 3.4% peak
  B=1, S=2048: 0.0053 ms, 417.3 GB/s, 5.2% peak
  B=2, S=128: 0.0038 ms, 73.3 GB/s, 0.9% peak
  B=2, S=256: 0.0038 ms, 146.0 GB/s, 1.8% peak
  B=2, S=512: 0.0041 ms, 271.4 GB/s, 3.4% peak
  B=2, S=1024: 0.0053 ms, 417.5 GB/s, 5.2% peak
  B=2, S=2048: 0.0077 ms, 578.4 GB/s, 7.2% peak
  B=4, S=128: 0.0038 ms, 146.0 GB/s, 1.8% peak
  B=4, S=256: 0.0041 ms, 271.5 GB/s, 3.4% peak
  B=4, S=512: 0.0053 ms, 417.4 GB/s, 5.2% peak
  B=4, S=1024: 0.0077 ms, 578.6 GB/s, 7.2% peak
  B=4, S=2048: 0.0129 ms, 688.4 GB/s, 8.6% peak
  B=8, S=128: 0.0041 ms, 271.5 GB/s, 3.4% peak
  B=8, S=256: 0.0053 ms, 417.4 GB/s, 5.2% peak
  B=8, S=512: 0.0077 ms, 578.6 GB/s, 7.2% peak
  B=8, S=1024: 0.0129 ms, 688.5 GB/s, 8.6% peak
  B=8, S=2048: 0.0228 ms, 780.2 GB/s, 9.8% peak
Saved ../results/topk_sigmoid.csv

[OK] topk_sigmoid completed successfully

======================================================================
Running: moe_fused_gate (MoE Routing, Memory)
======================================================================
============================================================
Benchmark: moe_fused_gate (Kernel #16)
============================================================

=== Decode Phase ===
  B=1: 0.0110 ms, 0.2 GB/s, 0.0% peak
  B=2: 0.0110 ms, 0.3 GB/s, 0.0% peak
  B=4: 0.0114 ms, 0.5 GB/s, 0.0% peak
  B=8: 0.0122 ms, 0.8 GB/s, 0.0% peak
  B=16: 0.0136 ms, 1.4 GB/s, 0.0% peak
  B=32: 0.0157 ms, 2.3 GB/s, 0.0% peak
  B=64: 0.0158 ms, 4.5 GB/s, 0.1% peak
  B=128: 0.0158 ms, 8.9 GB/s, 0.1% peak

=== Prefill Phase ===
  B=1, S=128: 0.0158 ms, 8.9 GB/s, 0.1% peak
  B=1, S=256: 0.0158 ms, 17.7 GB/s, 0.2% peak
  B=1, S=512: 0.0157 ms, 35.5 GB/s, 0.4% peak
  B=1, S=1024: 0.0157 ms, 70.8 GB/s, 0.9% peak
  B=1, S=2048: 0.0159 ms, 140.1 GB/s, 1.8% peak
  B=2, S=128: 0.0158 ms, 17.6 GB/s, 0.2% peak
  B=2, S=256: 0.0158 ms, 35.4 GB/s, 0.4% peak
  B=2, S=512: 0.0159 ms, 70.3 GB/s, 0.9% peak
  B=2, S=1024: 0.0159 ms, 140.4 GB/s, 1.8% peak
  B=2, S=2048: 0.0220 ms, 203.0 GB/s, 2.5% peak
  B=4, S=128: 0.0157 ms, 35.6 GB/s, 0.4% peak
  B=4, S=256: 0.0160 ms, 69.9 GB/s, 0.9% peak
  B=4, S=512: 0.0159 ms, 140.5 GB/s, 1.8% peak
  B=4, S=1024: 0.0220 ms, 202.7 GB/s, 2.5% peak
  B=4, S=2048: 0.0287 ms, 311.0 GB/s, 3.9% peak
  B=8, S=128: 0.0159 ms, 70.2 GB/s, 0.9% peak
  B=8, S=256: 0.0157 ms, 142.3 GB/s, 1.8% peak
  B=8, S=512: 0.0218 ms, 204.8 GB/s, 2.6% peak
  B=8, S=1024: 0.0288 ms, 309.9 GB/s, 3.9% peak
  B=8, S=2048: 0.0452 ms, 394.1 GB/s, 4.9% peak
Saved ../results/moe_fused_gate.csv

[OK] moe_fused_gate completed successfully

======================================================================
Running: prepare_moe_input (MoE, Memory)
======================================================================
============================================================
Benchmark: prepare_moe_input (Kernel #17)
============================================================

=== Decode Phase ===
  B=1: 0.0155 ms, 0.2 GB/s, 0.0% peak
  B=2: 0.0155 ms, 0.2 GB/s, 0.0% peak
  B=4: 0.0155 ms, 0.2 GB/s, 0.0% peak
  B=8: 0.0153 ms, 0.3 GB/s, 0.0% peak
  B=16: 0.0155 ms, 0.3 GB/s, 0.0% peak
  B=32: 0.0151 ms, 0.4 GB/s, 0.0% peak
  B=64: 0.0144 ms, 0.6 GB/s, 0.0% peak
  B=128: 0.0148 ms, 1.0 GB/s, 0.0% peak

=== Prefill Phase ===
  B=1, S=128: 0.0147 ms, 1.0 GB/s, 0.0% peak
  B=1, S=256: 0.0166 ms, 1.7 GB/s, 0.0% peak
  B=1, S=512: 0.0186 ms, 2.8 GB/s, 0.0% peak
  B=1, S=1024: 0.0221 ms, 4.6 GB/s, 0.1% peak
  B=1, S=2048: 0.0330 ms, 6.1 GB/s, 0.1% peak
  B=2, S=128: 0.0166 ms, 1.7 GB/s, 0.0% peak
  B=2, S=256: 0.0182 ms, 2.9 GB/s, 0.0% peak
  B=2, S=512: 0.0224 ms, 4.5 GB/s, 0.1% peak
  B=2, S=1024: 0.0332 ms, 6.0 GB/s, 0.1% peak
  B=2, S=2048: 0.0439 ms, 9.0 GB/s, 0.1% peak
  B=4, S=128: 0.0187 ms, 2.8 GB/s, 0.0% peak
  B=4, S=256: 0.0225 ms, 4.5 GB/s, 0.1% peak
  B=4, S=512: 0.0332 ms, 6.0 GB/s, 0.1% peak
  B=4, S=1024: 0.0454 ms, 8.7 GB/s, 0.1% peak
  B=4, S=2048: 0.0738 ms, 10.7 GB/s, 0.1% peak
  B=8, S=128: 0.0221 ms, 4.6 GB/s, 0.1% peak
  B=8, S=256: 0.0325 ms, 6.2 GB/s, 0.1% peak
  B=8, S=512: 0.0440 ms, 9.0 GB/s, 0.1% peak
  B=8, S=1024: 0.0742 ms, 10.6 GB/s, 0.1% peak
  B=8, S=2048: 0.1295 ms, 12.2 GB/s, 0.2% peak
Saved ../results/prepare_moe_input.csv

[OK] prepare_moe_input completed successfully

======================================================================
Running: scaled_fp4_experts_quant (MoE, Memory)
======================================================================
============================================================
Benchmark: scaled_fp4_experts_quant (Kernel #18)
============================================================

=== Decode Phase ===
  B=1: 0.0074 ms, 19.9 GB/s, 0.2% peak
  B=2: 0.0073 ms, 40.5 GB/s, 0.5% peak
  B=4: 0.0075 ms, 78.6 GB/s, 1.0% peak
  B=8: 0.0080 ms, 147.4 GB/s, 1.8% peak
  B=16: 0.0093 ms, 253.9 GB/s, 3.2% peak
  B=32: 0.0123 ms, 380.9 GB/s, 4.8% peak
  B=64: 0.0068 ms, 1391.8 GB/s, 17.4% peak
  B=128: 0.0109 ms, 1728.2 GB/s, 21.6% peak

=== Prefill Phase ===
  B=1, S=128: 0.0109 ms, 1730.6 GB/s, 21.6% peak
Warning: Kernel failed for B=1, S=256: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=1, S=512: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=1, S=1024: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=1, S=2048: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=2, S=128: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=2, S=256: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=2, S=512: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=2, S=1024: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=2, S=2048: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=4, S=128: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=4, S=256: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=4, S=512: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=4, S=1024: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=4, S=2048: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=8, S=128: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=8, S=256: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=8, S=512: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=8, S=1024: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=8, S=2048: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Saved ../results/scaled_fp4_experts_quant.csv

[OK] scaled_fp4_experts_quant completed successfully

======================================================================
Running: cutlass_fp4_group_mm (MoE, Compute)
======================================================================
============================================================
Benchmark: cutlass_fp4_group_mm (Kernel #19)
============================================================

=== Decode Phase: gate_up ===
  B=1: 0.0506 ms, 9290.8 GFLOPS, 0.10% peak
  B=2: 0.0531 ms, 17699.5 GFLOPS, 0.20% peak
  B=4: 0.0479 ms, 39205.0 GFLOPS, 0.44% peak
  B=8: 0.0503 ms, 74759.3 GFLOPS, 0.83% peak
  B=16: 0.0530 ms, 141684.4 GFLOPS, 1.57% peak
Warning: Kernel failed for B=32, S=1, op=gate_up: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=64, S=1, op=gate_up: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=128, S=1, op=gate_up: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


=== Decode Phase: down ===
  B=1: 0.0346 ms, 6785.3 GFLOPS, 0.08% peak
  B=2: 0.0340 ms, 13819.0 GFLOPS, 0.15% peak
  B=4: 0.0351 ms, 26769.7 GFLOPS, 0.30% peak
  B=8: 0.0348 ms, 53975.8 GFLOPS, 0.60% peak
  B=16: 0.0371 ms, 101203.6 GFLOPS, 1.12% peak
Warning: Kernel failed for B=32, S=1, op=down: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=64, S=1, op=down: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=128, S=1, op=down: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


=== Prefill Phase: gate_up ===
Warning: Kernel failed for B=1, S=128, op=gate_up: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=1, S=256, op=gate_up: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=1, S=512, op=gate_up: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=1, S=1024, op=gate_up: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=1, S=2048, op=gate_up: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=2, S=128, op=gate_up: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=2, S=256, op=gate_up: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=2, S=512, op=gate_up: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=2, S=1024, op=gate_up: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=2, S=2048, op=gate_up: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=4, S=128, op=gate_up: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=4, S=256, op=gate_up: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=4, S=512, op=gate_up: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=4, S=1024, op=gate_up: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=4, S=2048, op=gate_up: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=8, S=128, op=gate_up: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=8, S=256, op=gate_up: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=8, S=512, op=gate_up: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=8, S=1024, op=gate_up: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=8, S=2048, op=gate_up: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


=== Prefill Phase: down ===
Warning: Kernel failed for B=1, S=128, op=down: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=1, S=256, op=down: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=1, S=512, op=down: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=1, S=1024, op=down: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=1, S=2048, op=down: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=2, S=128, op=down: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=2, S=256, op=down: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=2, S=512, op=down: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=2, S=1024, op=down: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=2, S=2048, op=down: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=4, S=128, op=down: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=4, S=256, op=down: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=4, S=512, op=down: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=4, S=1024, op=down: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=4, S=2048, op=down: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=8, S=128, op=down: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=8, S=256, op=down: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=8, S=512, op=down: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=8, S=1024, op=down: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Warning: Kernel failed for B=8, S=2048, op=down: CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Saved ../results/cutlass_fp4_group_mm.csv

[OK] cutlass_fp4_group_mm completed successfully

======================================================================
Running: apply_shuffle_mul_sum (MoE, Memory)
======================================================================
============================================================
Benchmark: apply_shuffle_mul_sum (Kernel #20)
============================================================

=== Decode Phase ===
  B=1: 0.0029 ms, 43.9 GB/s, 0.5% peak
  B=2: 0.0030 ms, 86.6 GB/s, 1.1% peak
  B=4: 0.0029 ms, 175.2 GB/s, 2.2% peak
  B=8: 0.0030 ms, 349.8 GB/s, 4.4% peak
  B=16: 0.0030 ms, 690.5 GB/s, 8.6% peak
  B=32: 0.0030 ms, 1366.9 GB/s, 17.1% peak
  B=64: 0.0031 ms, 2703.8 GB/s, 33.8% peak
  B=128: 0.0031 ms, 5348.2 GB/s, 66.9% peak

=== Prefill Phase ===
  B=1, S=128: 0.0031 ms, 5349.2 GB/s, 66.9% peak
  B=1, S=256: 0.0042 ms, 7960.9 GB/s, 99.5% peak
  B=1, S=512: 0.0066 ms, 9965.6 GB/s, 124.6% peak
  B=1, S=1024: 0.0105 ms, 12616.4 GB/s, 157.7% peak
  B=1, S=2048: 0.0191 ms, 13829.4 GB/s, 172.9% peak
  B=2, S=128: 0.0041 ms, 7967.8 GB/s, 99.6% peak
  B=2, S=256: 0.0066 ms, 9968.6 GB/s, 124.6% peak
  B=2, S=512: 0.0105 ms, 12586.4 GB/s, 157.3% peak
  B=2, S=1024: 0.0191 ms, 13867.6 GB/s, 173.3% peak
  B=2, S=2048: 0.0380 ms, 13896.8 GB/s, 173.7% peak
  B=4, S=128: 0.0066 ms, 9948.9 GB/s, 124.4% peak
  B=4, S=256: 0.0105 ms, 12589.2 GB/s, 157.4% peak
  B=4, S=512: 0.0190 ms, 13902.3 GB/s, 173.8% peak
  B=4, S=1024: 0.0380 ms, 13909.7 GB/s, 173.9% peak
  B=4, S=2048: 0.0736 ms, 14377.3 GB/s, 179.7% peak
  B=8, S=128: 0.0105 ms, 12568.6 GB/s, 157.1% peak
  B=8, S=256: 0.0191 ms, 13871.0 GB/s, 173.4% peak
  B=8, S=512: 0.0377 ms, 14007.1 GB/s, 175.1% peak
  B=8, S=1024: 0.0735 ms, 14382.2 GB/s, 179.8% peak
  B=8, S=2048: 0.1434 ms, 14746.8 GB/s, 184.3% peak
Saved ../results/apply_shuffle_mul_sum.csv

[OK] apply_shuffle_mul_sum completed successfully

======================================================================
Running: moe_align_block_size (MoE, Memory)
======================================================================
============================================================
Benchmark: moe_align_block_size (Kernel #21)
============================================================

=== Decode Phase ===
  B=1: 0.0042 ms, 16.0 GB/s, 0.2% peak
  B=2: 0.0042 ms, 15.9 GB/s, 0.2% peak
  B=4: 0.0043 ms, 15.9 GB/s, 0.2% peak
  B=8: 0.0043 ms, 15.9 GB/s, 0.2% peak
  B=16: 0.0043 ms, 15.9 GB/s, 0.2% peak
  B=32: 0.0042 ms, 16.4 GB/s, 0.2% peak
  B=64: 0.0043 ms, 16.8 GB/s, 0.2% peak
  B=128: 0.0045 ms, 17.0 GB/s, 0.2% peak

=== Prefill Phase ===
  B=1, S=128: 0.0045 ms, 16.9 GB/s, 0.2% peak
  B=1, S=256: 0.0047 ms, 17.8 GB/s, 0.2% peak
  B=1, S=512: 0.0056 ms, 18.1 GB/s, 0.2% peak
  B=1, S=1024: 0.0072 ms, 18.6 GB/s, 0.2% peak
  B=1, S=2048: 0.0086 ms, 23.1 GB/s, 0.3% peak
  B=2, S=128: 0.0047 ms, 17.8 GB/s, 0.2% peak
  B=2, S=256: 0.0056 ms, 18.1 GB/s, 0.2% peak
  B=2, S=512: 0.0072 ms, 18.6 GB/s, 0.2% peak
  B=2, S=1024: 0.0086 ms, 23.1 GB/s, 0.3% peak
  B=2, S=2048: 0.0136 ms, 24.4 GB/s, 0.3% peak
  B=4, S=128: 0.0056 ms, 18.0 GB/s, 0.2% peak
  B=4, S=256: 0.0072 ms, 18.5 GB/s, 0.2% peak
  B=4, S=512: 0.0086 ms, 23.2 GB/s, 0.3% peak
  B=4, S=1024: 0.0135 ms, 24.5 GB/s, 0.3% peak
  B=4, S=2048: 0.0233 ms, 25.6 GB/s, 0.3% peak
  B=8, S=128: 0.0072 ms, 18.5 GB/s, 0.2% peak
  B=8, S=256: 0.0086 ms, 23.1 GB/s, 0.3% peak
  B=8, S=512: 0.0136 ms, 24.4 GB/s, 0.3% peak
  B=8, S=1024: 0.0233 ms, 25.6 GB/s, 0.3% peak
  B=8, S=2048: 0.0423 ms, 26.6 GB/s, 0.3% peak
Saved ../results/moe_align_block_size.csv

[OK] moe_align_block_size completed successfully

======================================================================
Running: trtllm_fp4_block_scale_moe (MoE, Mixed)
======================================================================
============================================================
Benchmark: trtllm_fp4_block_scale_moe (Kernel #22)
============================================================

=== Decode Phase ===
Warning: Kernel failed for B=1, S=1: Error in function 'run' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_batched_gemm_runner.cu:258: Error occurred when running GEMM! (numBatches:  256 , GemmMNK:  1   4096   7168 , Kernel:  bmm_E2m1_E2m1E2m1_Fp32_t128x8x512u2_s5_et128x8_m128x8x64_cga1x1x1_16dp256b_rM_TN_transOut_schedP2x1x2x3_bN_ldgsts_tmaOpt_clmp_swiGlu_dynBatch_sm100f )
Warning: Kernel failed for B=2, S=1: Error in function 'run' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_batched_gemm_runner.cu:258: Error occurred when running GEMM! (numBatches:  256 , GemmMNK:  2   4096   7168 , Kernel:  bmm_E2m1_E2m1E2m1_Fp32_t128x8x512u2_s5_et128x8_m128x8x64_cga1x1x1_16dp256b_rM_TN_transOut_schedP2x1x2x3_bN_ldgsts_tmaOpt_clmp_swiGlu_dynBatch_sm100f )
Warning: Kernel failed for B=4, S=1: Error in function 'run' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_batched_gemm_runner.cu:258: Error occurred when running GEMM! (numBatches:  256 , GemmMNK:  4   4096   7168 , Kernel:  bmm_E2m1_E2m1E2m1_Fp32_t128x8x512u2_s5_et128x8_m128x8x64_cga1x1x1_16dp256b_rM_TN_transOut_schedP2x1x2x3_bN_ldgsts_tmaOpt_clmp_swiGlu_dynBatch_sm100f )
Warning: Kernel failed for B=8, S=1: Error in function 'run' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_batched_gemm_runner.cu:258: Error occurred when running GEMM! (numBatches:  256 , GemmMNK:  8   4096   7168 , Kernel:  bmm_E2m1_E2m1E2m1_Fp32_t128x8x512u2_s5_et128x8_m128x8x64_cga1x1x1_16dp256b_rM_TN_transOut_schedP2x1x2x3_bN_ldgsts_tmaOpt_clmp_swiGlu_dynBatch_sm100f )
Warning: Kernel failed for B=16, S=1: Error in function 'run' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_batched_gemm_runner.cu:258: Error occurred when running GEMM! (numBatches:  256 , GemmMNK:  16   4096   7168 , Kernel:  bmm_E2m1_E2m1E2m1_Fp32_t128x8x512u2_s5_et128x8_m128x8x64_cga1x1x1_16dp256b_rM_TN_transOut_schedP2x1x2x3_bN_ldgsts_tmaOpt_clmp_swiGlu_dynBatch_sm100f )
Warning: Kernel failed for B=32, S=1: Error in function 'run' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_batched_gemm_runner.cu:258: Error occurred when running GEMM! (numBatches:  256 , GemmMNK:  32   4096   7168 , Kernel:  bmm_E2m1_E2m1E2m1_Fp32_t128x8x512u2_s5_et128x8_m128x8x64_cga1x1x1_16dp256b_rM_TN_transOut_schedP2x1x2x3_bN_ldgsts_tmaOpt_clmp_swiGlu_dynBatch_sm100f )
Warning: Kernel failed for B=64, S=1: Error in function 'run' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_batched_gemm_runner.cu:258: Error occurred when running GEMM! (numBatches:  256 , GemmMNK:  64   4096   7168 , Kernel:  bmm_E2m1_E2m1E2m1_Fp32_t128x8x512u2_s5_et128x8_m128x8x64_cga1x1x1_16dp256b_rM_TN_transOut_schedP2x1x2x3_bN_ldgsts_tmaOpt_clmp_swiGlu_dynBatch_sm100f )
Warning: Kernel failed for B=128, S=1: Error in function 'run' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_batched_gemm_runner.cu:258: Error occurred when running GEMM! (numBatches:  256 , GemmMNK:  128   4096   7168 , Kernel:  bmm_E2m1_E2m1E2m1_Fp32_t128x8x512u2_s5_et128x8_m128x8x64_cga1x1x1_16dp256b_rM_TN_transOut_schedP2x1x2x3_bN_ldgsts_tmaOpt_clmp_swiGlu_dynBatch_sm100f )

=== Prefill Phase ===
Warning: Kernel failed for B=1, S=128: Error in function 'run' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_batched_gemm_runner.cu:258: Error occurred when running GEMM! (numBatches:  256 , GemmMNK:  128   4096   7168 , Kernel:  bmm_E2m1_E2m1E2m1_Fp32_t128x8x512u2_s5_et128x8_m128x8x64_cga1x1x1_16dp256b_rM_TN_transOut_schedP2x1x2x3_bN_ldgsts_tmaOpt_clmp_swiGlu_dynBatch_sm100f )
Warning: Kernel failed for B=1, S=256: Error in function 'run' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_batched_gemm_runner.cu:258: Error occurred when running GEMM! (numBatches:  256 , GemmMNK:  256   4096   7168 , Kernel:  bmm_E2m1_E2m1E2m1_Fp32_t128x8x512u2_s5_et128x8_m128x8x64_cga1x1x1_16dp256b_rM_TN_transOut_schedP2x1x2x3_bN_ldgsts_tmaOpt_clmp_swiGlu_dynBatch_sm100f )
Warning: Kernel failed for B=1, S=512: Error in function 'run' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_batched_gemm_runner.cu:258: Error occurred when running GEMM! (numBatches:  256 , GemmMNK:  512   4096   7168 , Kernel:  bmm_E2m1_E2m1E2m1_Fp32_t128x8x512u2_s5_et128x8_m128x8x64_cga1x1x1_16dp256b_rM_TN_transOut_schedP2x1x2x3_bN_ldgsts_tmaOpt_clmp_swiGlu_dynBatch_sm100f )
Warning: Kernel failed for B=1, S=1024: Error in function 'run' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_batched_gemm_runner.cu:258: Error occurred when running GEMM! (numBatches:  256 , GemmMNK:  1024   4096   7168 , Kernel:  bmm_E2m1_E2m1E2m1_Fp32_t128x16x512u2_s5_et128x16_m128x16x64_cga1x1x1_16dp256b_rM_TN_transOut_schedP2x1x2x3_bN_ldgsts_tmaOpt_clmp_swiGlu_dynBatch_sm100f )
Warning: Kernel failed for B=1, S=2048: Error in function 'runImpl' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_fused_moe_routing_deepseek.cu:622: Got CUDA error. See above for details.
CUDA error in /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_fused_moe_routing_deepseek.cu:622 executing 'cudaLaunchKernelEx(&config, kernelTyped, params)': an illegal memory access was encounteredWarning: Kernel failed for B=2, S=128: Error in function 'run' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_batched_gemm_runner.cu:258: Error occurred when running GEMM! (numBatches:  256 , GemmMNK:  256   4096   7168 , Kernel:  bmm_E2m1_E2m1E2m1_Fp32_t128x8x512u2_s5_et128x8_m128x8x64_cga1x1x1_16dp256b_rM_TN_transOut_schedP2x1x2x3_bN_ldgsts_tmaOpt_clmp_swiGlu_dynBatch_sm100f )
Warning: Kernel failed for B=2, S=256: Error in function 'run' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_batched_gemm_runner.cu:258: Error occurred when running GEMM! (numBatches:  256 , GemmMNK:  512   4096   7168 , Kernel:  bmm_E2m1_E2m1E2m1_Fp32_t128x8x512u2_s5_et128x8_m128x8x64_cga1x1x1_16dp256b_rM_TN_transOut_schedP2x1x2x3_bN_ldgsts_tmaOpt_clmp_swiGlu_dynBatch_sm100f )
Warning: Kernel failed for B=2, S=512: Error in function 'run' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_batched_gemm_runner.cu:258: Error occurred when running GEMM! (numBatches:  256 , GemmMNK:  1024   4096   7168 , Kernel:  bmm_E2m1_E2m1E2m1_Fp32_t128x16x512u2_s5_et128x16_m128x16x64_cga1x1x1_16dp256b_rM_TN_transOut_schedP2x1x2x3_bN_ldgsts_tmaOpt_clmp_swiGlu_dynBatch_sm100f )
Warning: Kernel failed for B=2, S=1024: Error in function 'runImpl' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_fused_moe_routing_deepseek.cu:622: Got CUDA error. See above for details.
CUDA error in /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_fused_moe_routing_deepseek.cu:622 executing 'cudaLaunchKernelEx(&config, kernelTyped, params)': an illegal memory access was encounteredWarning: Kernel failed for B=2, S=2048: Error in function 'runImpl' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_fused_moe_routing_deepseek.cu:622: Got CUDA error. See above for details.
CUDA error in /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_fused_moe_routing_deepseek.cu:622 executing 'cudaLaunchKernelEx(&config, kernelTyped, params)': an illegal memory access was encounteredWarning: Kernel failed for B=4, S=128: Error in function 'run' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_batched_gemm_runner.cu:258: Error occurred when running GEMM! (numBatches:  256 , GemmMNK:  512   4096   7168 , Kernel:  bmm_E2m1_E2m1E2m1_Fp32_t128x8x512u2_s5_et128x8_m128x8x64_cga1x1x1_16dp256b_rM_TN_transOut_schedP2x1x2x3_bN_ldgsts_tmaOpt_clmp_swiGlu_dynBatch_sm100f )
Warning: Kernel failed for B=4, S=256: Error in function 'run' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_batched_gemm_runner.cu:258: Error occurred when running GEMM! (numBatches:  256 , GemmMNK:  1024   4096   7168 , Kernel:  bmm_E2m1_E2m1E2m1_Fp32_t128x16x512u2_s5_et128x16_m128x16x64_cga1x1x1_16dp256b_rM_TN_transOut_schedP2x1x2x3_bN_ldgsts_tmaOpt_clmp_swiGlu_dynBatch_sm100f )
Warning: Kernel failed for B=4, S=512: Error in function 'runImpl' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_fused_moe_routing_deepseek.cu:622: Got CUDA error. See above for details.
CUDA error in /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_fused_moe_routing_deepseek.cu:622 executing 'cudaLaunchKernelEx(&config, kernelTyped, params)': an illegal memory access was encounteredWarning: Kernel failed for B=4, S=1024: Error in function 'runImpl' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_fused_moe_routing_deepseek.cu:622: Got CUDA error. See above for details.
CUDA error in /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_fused_moe_routing_deepseek.cu:622 executing 'cudaLaunchKernelEx(&config, kernelTyped, params)': an illegal memory access was encounteredWarning: Kernel failed for B=4, S=2048: Error in function 'runImpl' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_fused_moe_routing_deepseek.cu:622: Got CUDA error. See above for details.
CUDA error in /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_fused_moe_routing_deepseek.cu:622 executing 'cudaLaunchKernelEx(&config, kernelTyped, params)': an illegal memory access was encounteredWarning: Kernel failed for B=8, S=128: Error in function 'run' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_batched_gemm_runner.cu:258: Error occurred when running GEMM! (numBatches:  256 , GemmMNK:  1024   4096   7168 , Kernel:  bmm_E2m1_E2m1E2m1_Fp32_t128x16x512u2_s5_et128x16_m128x16x64_cga1x1x1_16dp256b_rM_TN_transOut_schedP2x1x2x3_bN_ldgsts_tmaOpt_clmp_swiGlu_dynBatch_sm100f )
Warning: Kernel failed for B=8, S=256: Error in function 'runImpl' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_fused_moe_routing_deepseek.cu:622: Got CUDA error. See above for details.
CUDA error in /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_fused_moe_routing_deepseek.cu:622 executing 'cudaLaunchKernelEx(&config, kernelTyped, params)': an illegal memory access was encounteredWarning: Kernel failed for B=8, S=512: Error in function 'runImpl' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_fused_moe_routing_deepseek.cu:622: Got CUDA error. See above for details.
CUDA error in /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_fused_moe_routing_deepseek.cu:622 executing 'cudaLaunchKernelEx(&config, kernelTyped, params)': an illegal memory access was encounteredWarning: Kernel failed for B=8, S=1024: Error in function 'runImpl' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_fused_moe_routing_deepseek.cu:622: Got CUDA error. See above for details.
CUDA error in /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_fused_moe_routing_deepseek.cu:622 executing 'cudaLaunchKernelEx(&config, kernelTyped, params)': an illegal memory access was encounteredWarning: Kernel failed for B=8, S=2048: Error in function 'runImpl' at /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_fused_moe_routing_deepseek.cu:622: Got CUDA error. See above for details.
CUDA error in /usr/local/lib/python3.12/dist-packages/flashinfer/data/csrc/trtllm_fused_moe_routing_deepseek.cu:622 executing 'cudaLaunchKernelEx(&config, kernelTyped, params)': an illegal memory access was encountered
No results - kernel not available

[FAILED] trtllm_fp4_block_scale_moe: No CSV output (kernel not available or all runs failed)

======================================================================
Running: fused_moe_kernel (MoE, Mixed)
======================================================================
============================================================
Benchmark: fused_moe_kernel (Kernel #23)
============================================================

=== Decode Phase ===
  B=1: 0.1329 ms, 5302.8 GFLOPS, 2121.14% peak (memory)
  B=2: 0.2842 ms, 4958.6 GFLOPS, 991.73% peak (memory)
  B=4: 0.4828 ms, 5837.5 GFLOPS, 583.75% peak (memory)
  B=8: 0.8080 ms, 6976.9 GFLOPS, 348.85% peak (memory)
  B=16: 1.4569 ms, 7738.3 GFLOPS, 193.46% peak (memory)
  B=32: 2.2536 ms, 10005.4 GFLOPS, 125.07% peak (memory)
  B=64: 2.9935 ms, 15065.0 GFLOPS, 94.16% peak (memory)
  B=128: 3.4248 ms, 26335.3 GFLOPS, 82.31% peak (memory)

=== Prefill Phase ===
  B=1, S=128: 3.3724 ms, 26744.6 GFLOPS, 83.59% peak (memory)
  B=1, S=256: 3.5214 ms, 51226.7 GFLOPS, 80.07% peak (memory)
  B=1, S=512: 3.8955 ms, 92614.9 GFLOPS, 72.40% peak (memory)
  B=1, S=1024: 4.0820 ms, 176765.6 GFLOPS, 69.14% peak (memory)
  B=1, S=2048: 5.3470 ms, 269892.8 GFLOPS, 52.85% peak (memory)
  B=2, S=128: 3.5168 ms, 51294.0 GFLOPS, 80.17% peak (memory)
  B=2, S=256: 3.8909 ms, 92723.5 GFLOPS, 72.49% peak (memory)
  B=2, S=512: 4.0790 ms, 176894.4 GFLOPS, 69.19% peak (memory)
  B=2, S=1024: 5.3412 ms, 270182.8 GFLOPS, 52.91% peak (memory)
  B=2, S=2048: 8.2501 ms, 349840.5 GFLOPS, 34.34% peak (memory)
  B=4, S=128: 3.8870 ms, 92817.2 GFLOPS, 72.56% peak (memory)
  B=4, S=256: 4.0589 ms, 177770.8 GFLOPS, 69.53% peak (memory)
  B=4, S=512: 5.3270 ms, 270902.3 GFLOPS, 53.05% peak (memory)
  B=4, S=1024: 8.3470 ms, 345778.4 GFLOPS, 33.94% peak (memory)
  B=4, S=2048: 14.0306 ms, 411418.3 GFLOPS, 20.30% peak (memory)
  B=8, S=128: 4.0671 ms, 177411.3 GFLOPS, 69.39% peak (memory)
  B=8, S=256: 5.3051 ms, 272020.6 GFLOPS, 53.27% peak (memory)
  B=8, S=512: 8.1998 ms, 351987.1 GFLOPS, 34.55% peak (memory)
  B=8, S=1024: 14.1065 ms, 409205.0 GFLOPS, 20.19% peak (memory)
  B=8, S=2048: 32.0466 ms, 360252.2 GFLOPS, 16.01% peak (compute)
Saved ../results/fused_moe_kernel.csv

[2026-01-27 21:15:01] WARNING fused_moe_triton_config.py:127: Using default MoE kernel config. Performance might be sub-optimal! Config file not found at /lustre/fsw/sw_aidot/pengchengl/sglang_deepseek_only/mini-sglang-deepseek-only/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/E=256,N=2048,device_name=NVIDIA_GB200.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton
[2026-01-27 21:15:01] WARNING fused_moe_triton_config.py:119: Using MoE kernel config with down_moe=False. Performance might be sub-optimal! Config file not found at /lustre/fsw/sw_aidot/pengchengl/sglang_deepseek_only/mini-sglang-deepseek-only/sglang/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_5_1/E=256,N=2048,device_name=NVIDIA_GB200_down.json, you can create them with https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton

[OK] fused_moe_kernel completed successfully

Aggregated results saved to ../results/all_kernels.csv
Total benchmark results: 606

Summary saved to ../results/benchmark_summary.md

======================================================================
Benchmark Complete!
Successful: 22/23
Failed kernels:
  - trtllm_fp4_block_scale_moe: No CSV output (kernel not available or all runs failed)
======================================================================
