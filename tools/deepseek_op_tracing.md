# DeepSeek Op Tracing Tool

When user says "do deepseek op tracing", follow these steps to trace all ops in DeepSeek model execution order, mapping each op to its possible kernel implementations.

**Output:** `results/deepseek_op_traces.md` - A file containing ops in execution order with their kernel implementations.

**Input:** `results/deepseek_kernel_traces.md` - Kernel call chains (generated by `tools/deepseek_kernel_tracing.md`)

---

## ⚠️ PREREQUISITE CHECK

**Before running this tool, verify the kernel traces exist and are recent:**

```bash
# Check if kernel traces file exists
ls -la results/deepseek_kernel_traces.md

# Check the "Generated:" date in the file
head -10 results/deepseek_kernel_traces.md | grep "Generated:"
```

**If the file does not exist or is outdated:**
1. STOP this tool
2. Ask user: "The kernel traces file is missing or outdated. Should I run `do deepseek kernel tracing` first?"
3. Run `tools/deepseek_kernel_tracing.md` to generate fresh kernel traces
4. Then return to this tool

---

## ⚠️ CROSS-TOOL ALERTS

When running this tool, if you find issues that indicate problems in the **kernel traces**:

| Issue Found | Action |
|-------------|--------|
| Kernel trace ends at wrong op call site | **UPDATE kernel traces** - fix the call chain |
| Kernel trace missing intermediate steps | **UPDATE kernel traces** - add missing callers |
| Multiple kernels trace to different lines for same op | **UPDATE kernel traces** - verify which is correct |
| Kernel not found in kernel traces but should exist | **UPDATE kernel traces** - add the missing kernel |

**Alert format:**
```
⚠️ KERNEL TRACE ISSUE: [description]
Action: Update results/deepseek_kernel_traces.md - [specific fix needed]
```

---

## Key Concept: Op vs Kernel

- **Op (Operation):** A logical operation in the model (e.g., `self.self_attn()`, `self.mlp()`, `self.input_layernorm()`)
  - Call site is in DeepSeek model files (deepseek_v2.py, deepseek.py, etc.)
  - Represents WHAT the model does

- **Kernel:** A specific implementation of an op (e.g., cutlass_mla_decode, trtllm_batch_decode, triton fused_moe)
  - Multiple kernels may implement the same op (different backends)
  - Represents HOW the op is executed

---

## Relationship to Kernel Tracing

The **kernel traces** (`results/deepseek_kernel_traces.md`) trace FROM kernel call sites BACK TO op call sites.

The **op traces** (this tool) organize the same information FROM op call sites DOWN TO kernels.

```
Kernel Tracing (bottom-up):          Op Tracing (top-down):

kernel_call_site                     op_call_site (deepseek_v2.py)
└─> intermediate                     ├─> kernel_1 (cutlass)
└──> intermediate                    ├─> kernel_2 (trtllm)
└───> op_call_site                   └─> kernel_3 (triton)
```

**Cross-validation:**
- Every kernel's final call chain entry should be an op call site
- Every op should have at least one kernel
- Group kernels by their terminating op call site

---

## Output Format

The output has two parts:

### Part 1: Execution Graph

Shows ops and sub-ops as a directed graph with clickable links for leaf ops only.

**Format rules:**
- **Container functions** (non-ops): No links - e.g., `self.model()`, `self.self_attn()`, `self.mlp()`
- **Leaf ops** (with kernels): Clickable links - e.g., `[self.input_layernorm()](path#Lline)`
- **Path annotations**: Bold format - e.g., `**(MHA path)**`, `**(MLA path)**`

```
self.model()
└─> layer()
    ├─> self.layer_communicator.prepare_attn()
    │   └─> [self.input_layernorm()](../sglang/python/sglang/srt/layers/layernorm.py#L102)
    ├─> self.self_attn()
    │   ├─> self.forward_prepare()
    │   │   ├─> self.forward_normal_prepare() **(MHA path)**
    │   │   │   ├─> [self.fused_qkv_a_proj_with_mqa()](../sglang/python/sglang/srt/models/deepseek_v2.py#L1300)
    │   │   │   ├─> [self.q_a_layernorm()](../sglang/python/sglang/srt/models/deepseek_v2.py#L1323)
    │   │   │   └─> ...
    │   │   ├─> self.forward_absorb_prepare() **(MLA path)**
    │   │   │   └─> ...
    │   │   └─> self.forward_absorb_fused_mla_rope_prepare() **(MLA_FUSED_ROPE path)**
    │   │       └─> ...
    │   └─> self.forward_core()
    │       └─> ...
    └─> self.mlp()
        ├─> [Dense MLP - DeepseekV2MLP]
        │   ├─> [self.gate_up_proj()](../sglang/python/sglang/srt/models/deepseek_v2.py#L271)
        │   └─> ...
        └─> [MoE MLP - DeepseekV2MoE]
            └─> ...
```

### Part 2: Op Summary Table

A table listing all ops with their kernel counts and primary kernels:

```
| Op | Kernel Count | Primary Kernels |
|----|--------------|-----------------|
| `self.input_layernorm()` | 2 | rmsnorm, fused_add_rmsnorm |
| `self.attn_mqa()` | 3 | cutlass_mla_decode, trtllm_batch_decode_with_kv_cache_mla, ... |
```

**Note:** Do NOT include container ops (self.model(), self.mlp(), self.self_attn()) in the summary table.

**Link format:** `[function_name()](path#Lline)` - clicking the function name takes you to its call site.

---

## Step 1: Build Op Execution Graph

Generate a directed graph of ops/sub-ops from the model's forward pass. The graph captures execution order and branching paths (different cases).

### 1.1 Find forward functions

```bash
grep -n "def forward" sglang/python/sglang/srt/models/deepseek_v2.py
```

### 1.2 Trace execution flow

Read the forward methods and trace which ops/sub-ops are called. Build a directed graph where:
- **Nodes** are `self.xxx()` calls (ops/sub-ops) AND intermediate wrapper functions
- **Edges** represent parent-child relationships (which op contains which sub-ops)
- Include intermediate functions like `layer()`, `forward_normal()`, `forward_prepare()` to show structure
- Only add **clickable links** to leaf ops (ops that have kernels)

### 1.3 Graph format

Include both container functions (no links) and leaf ops (with links):

```
self.model()
└─> layer()
    ├─> self.layer_communicator.prepare_attn()
    │   └─> [self.input_layernorm()](path#Lline)
    ├─> self.self_attn()
    │   ├─> self.forward_prepare()
    │   │   ├─> self.forward_normal_prepare() **(MHA path)**
    │   │   │   └─> [self.op_a()](path#Lline)
    │   │   └─> self.forward_absorb_prepare() **(MLA path)**
    │   │       └─> [self.op_b()](path#Lline)
    │   └─> self.forward_core()
    │       └─> ...
    └─> self.mlp()
```

**Tree characters:**
- `└─>` - last/only child at this level
- `├─>` - sibling with more siblings below
- `│` - vertical connector between siblings

Note: Different cases (MoE vs dense, different backends) are shown with **bold path annotations**.

### 1.4 Discover ops from model code

The graph is built by reading the model code, NOT from hardcoded lists. Read the forward methods to find `self.xxx()` calls - those are the ops to include in the graph.

---

## Step 2: Find Kernels for Each Op

For each op discovered in Step 1, find its kernels from `results/deepseek_kernel_traces.md`.

### 2.1 Search kernel traces for op

For each op in the execution graph, search the kernel traces file to find kernels that trace to it:

```bash
# Example: find kernels for self.self_attn()
grep -B 20 "self\.self_attn()" results/deepseek_kernel_traces.md
```

### 2.2 Extract kernel from call chain

Each kernel trace shows a call chain ending at an op. The first line is the kernel:

```
[kernel_a()](path#Lline) *(source)*   ← KERNEL
└─> ...
└──> [self.some_op()](...) ← OP
```

Collect all kernels that trace to each op.

---

## Step 3: Document Kernel Variants

For ops with multiple kernels, note them as alternative implementations:
- Group by kernel source: *(sgl-kernel)*, *(flashinfer)*, *(triton)*
- Group by phase if applicable: decode vs prefill

---

## Step 4: Generate Output File

Create `results/deepseek_op_traces.md` with:

1. **Execution Graph** - Directed graph with clickable links for leaf ops only
2. **Op Summary Table** - List of leaf ops with kernel counts (no containers)
3. **Cross-Validation Summary** - Confirmation that all kernels are mapped
4. **Notes** - Context about container ops, quantization variants, backends

**Note:** Do NOT include a detailed "Op-to-Kernel Mapping" section - the kernel traces file already has this information with full call chains.

### Output Structure

```markdown
# DeepSeek Op Traces

## Execution Graph

self.model()
└─> layer()
    ├─> self.layer_communicator.prepare_attn()
    │   └─> [self.input_layernorm()](../sglang/python/sglang/srt/layers/layernorm.py#L102)
    ├─> self.self_attn()
    │   ├─> self.forward_prepare()
    │   │   ├─> self.forward_normal_prepare() **(MHA path)**
    │   │   │   └─> [self.op_a()](path#Lline)
    │   │   └─> ...
    │   └─> ...
    └─> self.mlp()
        └─> ...

## Op Summary

| Op | Kernel Count | Primary Kernels |
|----|--------------|-----------------|
| `self.input_layernorm()` | 2 | rmsnorm, fused_add_rmsnorm |
| `self.attn_mqa()` | 3 | cutlass_mla_decode, trtllm_batch_decode, ... |

## Cross-Validation Summary

✅ All kernels from `results/deepseek_kernel_traces.md` are mapped to ops above.

| Category | Kernels Mapped |
|----------|----------------|
| Attention | N (...) |
| MoE | N (...) |

## Notes

- **Container ops** like `self.model()`, `self.mlp()` don't have direct kernels
- **Quantization variants** determine which GEMM kernel is used
- ...
```

---

## Step 5: Cross-Validation with Kernel Traces

After generating the op trace file, validate against kernel traces:

### 5.1 Every Kernel Should Map to an Op

```bash
# Extract all kernels from kernel traces
grep -E "^\[.*\] \`[a-z_]+\(\)\`" results/deepseek_kernel_traces.md | head -20

# Verify each kernel appears under some op in op traces
```

**If kernel not mapped to any op:**
```
⚠️ KERNEL TRACE ISSUE: Kernel `xxx` not mapped to any op
Action: Update results/deepseek_kernel_traces.md - verify call chain ends at valid op
```

### 5.2 Every Op Should Have Kernels

```bash
# List ops from op traces
grep -E "^### [0-9]+\. " results/deepseek_op_traces.md

# Verify each has at least one kernel listed
```

**If op has no kernels:**
```
⚠️ KERNEL TRACE ISSUE: Op `xxx` has no kernels traced to it
Action: Update results/deepseek_kernel_traces.md - add missing kernel traces for this op
```

### 5.3 Call Chain Consistency

For any kernel, the op it maps to should match the last entry in its kernel trace call chain:

```
Kernel trace for kernel_x:
  ... └──────> [deepseek_v2.py:xxx] self.some_op()  <-- This op

Op trace for self.some_op():
  Kernels: kernel_x, ...  <-- Should include this kernel
```

**If mismatch found:**
```
⚠️ KERNEL TRACE ISSUE: Kernel `xxx` traces to line Y but op is at line Z
Action: Update results/deepseek_kernel_traces.md - fix call chain to correct op call site
```

### 5.4 Summary of Discrepancies

At the end of cross-validation, output a summary:

```
## Cross-Validation Summary

✅ Passed: X kernels correctly mapped to ops
⚠️ Issues found: Y

Issues to fix in kernel traces:
1. [issue description]
2. [issue description]
```

---

## DeepSeek Model Files

- **DeepSeek V2/V3/R1:** `sglang/python/sglang/srt/models/deepseek_v2.py`
- **DeepSeek V1:** `sglang/python/sglang/srt/models/deepseek.py`
- **DeepSeek NextN:** `sglang/python/sglang/srt/models/deepseek_nextn.py`
- **Common components:** `sglang/python/sglang/srt/models/deepseek_common/`

---

## Notes

- **Ops are discovered from kernel traces** - do not pre-specify op names
- **Link format:** `[function_name()](../path/file.py#Lline)` - function name is clickable, links to call site
- **Only leaf ops get links** - container functions (self.model(), self.mlp(), etc.) have no links
- **Path annotations in bold:** `**(MHA path)**`, `**(MLA path)**`, `**(MLA_FUSED_ROPE path)**`
- **No Op-to-Kernel Mapping section** - kernel traces already have detailed call chains
- **Sub-ops** are discovered through the call chain hierarchy in kernel traces
